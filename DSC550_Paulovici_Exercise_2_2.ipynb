{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Week 2:\n",
    " File: DSC550_Paulovici_Exercise_2_2.py (.ipynb)<br>\n",
    " Name: Kevin Paulovici<br>\n",
    " Date: 3/22/2020<br>\n",
    " Course: DSC 550 Data Mining (2205-1)<br>\n",
    " Assignment: 2.2 Exercise: Build your Text Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Part 1\n",
    " You can find the data for this exercise in the Weekly Resources: Week 2 Data Files.Preparing Text: For this part, you will start by reading the Income.json (controversial-comments.json) file into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>con</th>\n",
       "      <th>txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Well it's great that he did something about th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>You are right Mr. President.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>You have given no input apart from saying I am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>I get the frustration but the reason they want...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I am far from an expert on TPP and I would ten...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   con                                                txt\n",
       "0    0  Well it's great that he did something about th...\n",
       "1    0                       You are right Mr. President.\n",
       "2    0  You have given no input apart from saying I am...\n",
       "3    0  I get the frustration but the reason they want...\n",
       "4    0  I am far from an expert on TPP and I would ten..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "with open(\"controversial-comments_subset.json\") as fileIn:\n",
    "    count = 0\n",
    "\n",
    "    # read each line as a json obj\n",
    "    for line in fileIn:\n",
    "        line = line.strip()\n",
    "        data = json.loads(line)\n",
    "\n",
    "        # create the dataframe for the first obj else append data\n",
    "        if count == 0:\n",
    "            df = json_normalize(data)            \n",
    "        else:\n",
    "            df2 = json_normalize(data)\n",
    "            df = df.append(df2, ignore_index=True)\n",
    "        \n",
    "        count += 1\n",
    "\n",
    "df.head(5)        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Part A\n",
    " Convert all text to lowercase letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>con</th>\n",
       "      <th>txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>well it's great that he did something about th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>you are right mr. president.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>you have given no input apart from saying i am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>i get the frustration but the reason they want...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i am far from an expert on tpp and i would ten...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   con                                                txt\n",
       "0    0  well it's great that he did something about th...\n",
       "1    0                       you are right mr. president.\n",
       "2    0  you have given no input apart from saying i am...\n",
       "3    0  i get the frustration but the reason they want...\n",
       "4    0  i am far from an expert on tpp and i would ten..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"txt\"] = df[\"txt\"].str.lower()\n",
    "df.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Part B\n",
    " Remove all punctuation from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>con</th>\n",
       "      <th>txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>well its great that he did something about tho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>you are right mr president</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>you have given no input apart from saying i am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>i get the frustration but the reason they want...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i am far from an expert on tpp and i would ten...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   con                                                txt\n",
       "0    0  well its great that he did something about tho...\n",
       "1    0                         you are right mr president\n",
       "2    0  you have given no input apart from saying i am...\n",
       "3    0  i get the frustration but the reason they want...\n",
       "4    0  i am far from an expert on tpp and i would ten..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "import sys\n",
    "\n",
    "punctuation = dict.fromkeys(i for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith('P'))\n",
    "\n",
    "df.txt = df.txt.apply(lambda x: x.translate(punctuation))\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Part C\n",
    " Remove stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>con</th>\n",
       "      <th>txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>well great something beliefs office doubt trum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>right mr president</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>given input apart saying wrong argument clearly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>get frustration reason want way foundation com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>far expert tpp would tend agree lot problems u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   con                                                txt\n",
       "0    0  well great something beliefs office doubt trum...\n",
       "1    0                                 right mr president\n",
       "2    0    given input apart saying wrong argument clearly\n",
       "3    0  get frustration reason want way foundation com...\n",
       "4    0  far expert tpp would tend agree lot problems u..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# load stop words\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# TODO: get lambda function working\n",
    "# remove stop_words\n",
    "# df.txt.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "\n",
    "# remove stop words\n",
    "def stopWords(sentence):\n",
    "    tokens = sentence.split()\n",
    "    w = [item for item in tokens if item not in stop_words]\n",
    "    return ' '.join(w)\n",
    "\n",
    "df.txt = df.txt.apply(stopWords)\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Part D\n",
    " Apply NLTKâ€™s PorterStemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>con</th>\n",
       "      <th>txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>well great someth belief offic doubt trump wou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>right mr presid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>given input apart say wrong argument clearli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>get frustrat reason want way foundat complex p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>far expert tpp would tend agre lot problem und...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   con                                                txt\n",
       "0    0  well great someth belief offic doubt trump wou...\n",
       "1    0                                    right mr presid\n",
       "2    0       given input apart say wrong argument clearli\n",
       "3    0  get frustrat reason want way foundat complex p...\n",
       "4    0  far expert tpp would tend agre lot problem und..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# creater stemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# TODO: get lambda function working\n",
    "# df.txt.apply(lambda x: [porter.stem(word) for word in x])\n",
    "\n",
    "def stem_sentences(sentence):\n",
    "    tokens = sentence.split()\n",
    "    stemmed_tokens = [porter.stem(token) for token in tokens]\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "df.txt = df.txt.apply(stem_sentences)\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Part E\n",
    " Use a Tf-idf vector instead of the word frequency vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 158)\t0.1922649867231423\n",
      "  (0, 64)\t0.2383074953261643\n",
      "  (0, 133)\t0.3845299734462846\n",
      "  (0, 15)\t0.2383074953261643\n",
      "  (0, 100)\t0.2383074953261643\n",
      "  (0, 42)\t0.2383074953261643\n",
      "  (0, 147)\t0.1922649867231423\n",
      "  (0, 164)\t0.1595973069871517\n",
      "  (0, 52)\t0.2383074953261643\n",
      "  (0, 149)\t0.2383074953261643\n",
      "  (0, 71)\t0.2383074953261643\n",
      "  (0, 113)\t0.3845299734462846\n",
      "  (0, 65)\t0.2383074953261643\n",
      "  (0, 99)\t0.1922649867231423\n",
      "  (0, 31)\t0.2383074953261643\n",
      "  (0, 155)\t0.2383074953261643\n",
      "  (1, 119)\t0.49552379079705033\n",
      "  (1, 92)\t0.6141889663426562\n",
      "  (1, 107)\t0.6141889663426562\n",
      "  (2, 61)\t0.3128396318588854\n",
      "  (2, 74)\t0.38775666010579296\n",
      "  (2, 7)\t0.38775666010579296\n",
      "  (2, 123)\t0.38775666010579296\n",
      "  (2, 165)\t0.38775666010579296\n",
      "  (2, 9)\t0.38775666010579296\n",
      "  :\t:\n",
      "  (4, 47)\t0.06913927585604182\n",
      "  (4, 159)\t0.06913927585604182\n",
      "  (4, 67)\t0.06913927585604182\n",
      "  (4, 44)\t0.06913927585604182\n",
      "  (4, 11)\t0.06913927585604182\n",
      "  (4, 96)\t0.06913927585604182\n",
      "  (4, 91)\t0.06913927585604182\n",
      "  (4, 19)\t0.06913927585604182\n",
      "  (4, 58)\t0.06913927585604182\n",
      "  (4, 45)\t0.06913927585604182\n",
      "  (4, 160)\t0.06913927585604182\n",
      "  (4, 120)\t0.06913927585604182\n",
      "  (4, 8)\t0.06913927585604182\n",
      "  (4, 48)\t0.06913927585604182\n",
      "  (4, 128)\t0.06913927585604182\n",
      "  (4, 98)\t0.06913927585604182\n",
      "  (4, 55)\t0.06913927585604182\n",
      "  (4, 118)\t0.06913927585604182\n",
      "  (4, 40)\t0.06913927585604182\n",
      "  (4, 38)\t0.06913927585604182\n",
      "  (4, 131)\t0.06913927585604182\n",
      "  (4, 117)\t0.06913927585604182\n",
      "  (4, 95)\t0.06913927585604182\n",
      "  (4, 112)\t0.06913927585604182\n",
      "  (4, 62)\t0.06913927585604182\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.2383075  0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.2383075  0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.2383075  0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.2383075  0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.2383075  0.2383075\n",
      "  0.         0.         0.         0.         0.         0.2383075\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.19226499 0.2383075  0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.38452997\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.38452997 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.19226499 0.         0.2383075\n",
      "  0.         0.         0.         0.         0.         0.2383075\n",
      "  0.         0.         0.19226499 0.         0.         0.\n",
      "  0.         0.         0.15959731 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.61418897 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.61418897\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.49552379\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.38775666 0.         0.38775666 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.38775666 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.31283963 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.38775666 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.38775666 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.38775666 0.        ]\n",
      " [0.         0.12077036 0.         0.         0.         0.\n",
      "  0.12077036 0.         0.         0.         0.12077036 0.\n",
      "  0.12077036 0.12077036 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.12077036 0.         0.\n",
      "  0.         0.         0.12077036 0.12077036 0.12077036 0.\n",
      "  0.         0.         0.12077036 0.         0.         0.\n",
      "  0.12077036 0.         0.         0.12077036 0.         0.12077036\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.12077036\n",
      "  0.         0.         0.         0.12077036 0.         0.12077036\n",
      "  0.36231108 0.         0.         0.24154072 0.         0.\n",
      "  0.         0.         0.         0.         0.12077036 0.\n",
      "  0.         0.         0.         0.         0.12077036 0.12077036\n",
      "  0.         0.         0.29231029 0.         0.         0.\n",
      "  0.36231108 0.12077036 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.12077036 0.\n",
      "  0.09743676 0.         0.12077036 0.         0.         0.09743676\n",
      "  0.12077036 0.         0.         0.         0.         0.09743676\n",
      "  0.         0.12077036 0.         0.         0.12077036 0.\n",
      "  0.         0.         0.         0.12077036 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.12077036\n",
      "  0.         0.12077036 0.12077036 0.         0.         0.12077036\n",
      "  0.         0.         0.09743676 0.         0.12077036 0.\n",
      "  0.09743676 0.         0.         0.         0.         0.\n",
      "  0.12077036 0.24154072 0.         0.         0.         0.\n",
      "  0.12077036 0.         0.08088132 0.         0.        ]\n",
      " [0.06913928 0.         0.06913928 0.06913928 0.06913928 0.06913928\n",
      "  0.         0.         0.06913928 0.         0.         0.06913928\n",
      "  0.         0.         0.06913928 0.         0.06913928 0.06913928\n",
      "  0.06913928 0.06913928 0.06913928 0.         0.06913928 0.06913928\n",
      "  0.2765571  0.         0.         0.         0.         0.06913928\n",
      "  0.13827855 0.         0.         0.06913928 0.06913928 0.06913928\n",
      "  0.         0.06913928 0.06913928 0.         0.06913928 0.\n",
      "  0.         0.13827855 0.06913928 0.06913928 0.06913928 0.06913928\n",
      "  0.06913928 0.06913928 0.06913928 0.06913928 0.         0.\n",
      "  0.06913928 0.06913928 0.06913928 0.         0.06913928 0.\n",
      "  0.         0.05578113 0.06913928 0.         0.         0.\n",
      "  0.13827855 0.06913928 0.06913928 0.06913928 0.         0.\n",
      "  0.06913928 0.13827855 0.         0.20741783 0.         0.\n",
      "  0.06913928 0.06913928 0.05578113 0.06913928 0.20741783 0.13827855\n",
      "  0.         0.         0.06913928 0.06913928 0.13827855 0.06913928\n",
      "  0.06913928 0.06913928 0.         0.06913928 0.06913928 0.06913928\n",
      "  0.06913928 0.06913928 0.06913928 0.1673434  0.         0.06913928\n",
      "  0.06913928 0.06913928 0.06913928 0.06913928 0.         0.\n",
      "  0.05578113 0.20741783 0.         0.06913928 0.06913928 0.\n",
      "  0.         0.13827855 0.06913928 0.06913928 0.06913928 0.\n",
      "  0.06913928 0.         0.06913928 0.         0.         0.06913928\n",
      "  0.06913928 0.20741783 0.06913928 0.         0.06913928 0.06913928\n",
      "  0.06913928 0.11156226 0.06913928 0.06913928 0.06913928 0.\n",
      "  0.13827855 0.         0.         0.06913928 0.06913928 0.\n",
      "  0.2765571  0.06913928 0.05578113 0.05578113 0.         0.\n",
      "  0.05578113 0.13827855 0.06913928 0.06913928 0.06913928 0.\n",
      "  0.         0.         0.05578113 0.06913928 0.06913928 0.06913928\n",
      "  0.         0.06913928 0.18521352 0.         0.06913928]]\n",
      "{'well': 158, 'great': 64, 'someth': 133, 'belief': 15, 'offic': 100, 'doubt': 42, 'trump': 147, 'would': 164, 'fight': 52, 'un': 149, 'im': 71, 'realli': 113, 'happi': 65, 'obama': 99, 'couldoh': 31, 'wait': 155, 'right': 119, 'mr': 92, 'presid': 107, 'given': 61, 'input': 74, 'apart': 7, 'say': 123, 'wrong': 165, 'argument': 9, 'clearli': 25, 'get': 60, 'frustrat': 59, 'reason': 114, 'want': 156, 'way': 157, 'foundat': 57, 'complex': 28, 'problem': 108, 'advanc': 1, 'grade': 63, 'decent': 36, 'sat': 121, 'type': 148, 'test': 139, 'math': 84, 'dont': 41, 'understand': 150, 'lot': 80, 'mathemat': 85, 'answer': 6, 'time': 143, 'figur': 53, 'common': 27, 'sens': 129, 'work': 162, 'around': 10, 'question': 110, 'ill': 70, 'prepar': 106, 'take': 137, 'colleg': 26, 'level': 77, 'cours': 32, 'despit': 39, 'averag': 12, 'score': 124, 'theyr': 140, 'tri': 146, 'bust': 21, 'kid': 76, 'ball': 13, 'far': 50, 'expert': 49, 'tpp': 144, 'tend': 138, 'agre': 2, 'push': 109, 'creat': 33, 'econom': 43, 'bulwark': 20, 'china': 24, 'pacif': 102, 'administr': 0, 'recogn': 115, 'increas': 73, 'strength': 136, 'matur': 86, 'bellicos': 16, 'see': 126, 'south': 134, 'sea': 125, 'us': 151, 'alli': 4, 'penetr': 103, 'mani': 82, 'emerg': 46, 'market': 83, 'otherwis': 101, 'natur': 97, 'align': 3, 'alway': 5, 'thought': 141, 'curiou': 35, 'critiqu': 34, 'hardli': 66, 'saw': 122, 'mention': 89, 'track': 145, 'record': 116, 'someon': 132, 'railroad': 111, 'worker': 163, 'corpor': 29, 'interest': 75, 'must': 94, 'felt': 51, 'huge': 69, 'import': 72, 'use': 152, 'much': 93, 'polit': 105, 'capit': 22, 'like': 78, 'mayb': 88, 'could': 30, 'better': 18, 'certainli': 23, 'seem': 127, 'yet': 166, 'best': 17, 'manag': 81, 'vari': 153, 'play': 104, 'howev': 68, 'strateg': 135, 'throw': 142, 'begin': 14, 'signific': 130, 'withdraw': 161, 'foreign': 56, 'definit': 37, 'move': 90, 'fill': 54, 'void': 154, 'may': 87, 'look': 79, 'end': 47, 'western': 159, 'hegemoni': 67, 'elect': 44, 'ascend': 11, 'nationalist': 96, 'movement': 91, 'britain': 19, 'franc': 58, 'elsewher': 45, 'what': 160, 'sad': 120, 'appear': 8, 'entir': 48, 'selfinflict': 128, 'noth': 98, 'forc': 55, 'retract': 118, 'develop': 40, 'democraci': 38, 'simpli': 131, 'result': 117, 'myopic': 95, 'reaction': 112, 'global': 62}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "text_data = df.txt\n",
    "tfidf = TfidfVectorizer()\n",
    "feature_matrix = tfidf.fit_transform(text_data)\n",
    "\n",
    "print(feature_matrix)\n",
    "print(feature_matrix.toarray())\n",
    "print(tfidf.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Part 2\n",
    " Complete the 5.3 Encoding Dictionaries of Features examples. Be sure to read the Discussion concerning keeping track of how many times a word is used in a document. Also be sure to run the example and read the Discussion from 6.9 Weighting Word Importance. Finally, consider tokenizing words or sentences (see 6.4) and tagging parts of speech (see 6.7) Be sure to review how to encode days of the week (see 7.6). <br><br>\n",
    " You can start with the #1 program and add to it or you can start a new program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 5.3 Encoding Dictionaries of Features - book examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4., 2., 0.],\n",
       "       [3., 4., 0.],\n",
       "       [0., 1., 2.],\n",
       "       [0., 2., 2.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# create dict\n",
    "data_dict = [{\"Red\": 2, \"Blue\": 4},\n",
    "             {\"Red\": 4, \"Blue\": 3},\n",
    "             {\"Red\": 1, \"Yellow\": 2},\n",
    "             {\"Red\": 2, \"Yellow\": 2}]\n",
    "\n",
    "# create dict vectorizer\n",
    "dictvectorizer = DictVectorizer(sparse=False)\n",
    "\n",
    "# convert dict to feature matrix\n",
    "features = dictvectorizer.fit_transform(data_dict)\n",
    "\n",
    "# view feature matrix\n",
    "features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Blue', 'Red', 'Yellow']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get feature names\n",
    "feature_names = dictvectorizer.get_feature_names()\n",
    "\n",
    "# view feature names\n",
    "feature_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Blue</th>\n",
       "      <th>Red</th>\n",
       "      <th>Yellow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Blue  Red  Yellow\n",
       "0   4.0  2.0     0.0\n",
       "1   3.0  4.0     0.0\n",
       "2   0.0  1.0     2.0\n",
       "3   0.0  2.0     2.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dataframe to output data better\n",
    "import pandas as pd\n",
    "\n",
    "# create df from features\n",
    "pd.DataFrame(features, columns=feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4., 2., 0.],\n",
       "       [3., 4., 0.],\n",
       "       [0., 1., 2.],\n",
       "       [0., 2., 2.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create word counts dictionaries for four documents\n",
    "doc_1_word_count = {\"Red\": 2, \"Blue\": 4}\n",
    "doc_2_word_count = {\"Red\": 4, \"Blue\": 3}\n",
    "doc_3_word_count = {\"Red\": 1, \"Yellow\": 2}\n",
    "doc_4_word_count = {\"Red\": 2, \"Yellow\": 2}\n",
    "\n",
    "# create list \n",
    "doc_word_counts = [doc_1_word_count, doc_2_word_count, doc_3_word_count, doc_4_word_count]\n",
    "\n",
    "# convert list of word count dicts into feature matrix\n",
    "dictvectorizer.fit_transform(doc_word_counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 6.4 Tokenizing Text - book examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'science', 'of', 'today', 'is', 'the', 'technology', 'of', 'tomorrow']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# create text\n",
    "string = \"The science of today is the technology of tomorrow\"\n",
    "\n",
    "# tokenize words\n",
    "word_tokenize(string) # error\n",
    "\n",
    "# work around for viewing only\n",
    "# string.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The science of today is the technology of tomorrow.', 'Tomrow is today.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize sentences\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# create text\n",
    "string = \"The science of today is the technology of tomorrow. Tomrow is today.\"\n",
    "\n",
    "# tokenize sentences\n",
    "sent_tokenize(string) # error\n",
    "\n",
    "# work around for viewing only\n",
    "# string.split(\".\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 6.7 Tagging Parts of Speech - book examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chris']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# create text\n",
    "text_data = \"Chris loved outdoor running\"\n",
    "\n",
    "# use pre-trained part of speech tagger\n",
    "text_tagged = pos_tag(word_tokenize(text_data))\n",
    "\n",
    "# show parts of speech\n",
    "text_tagged\n",
    "\n",
    "[word for word, tag in text_tagged if tag in ['NN', 'NNS', 'NNP', 'NNPS']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 1, 0, 1, 1, 1, 0],\n",
       "       [1, 0, 1, 1, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 1, 1, 1, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# create text\n",
    "tweets = [\"I am eating a burrito for breakfast\", \n",
    "\"Political science is an amazing field\", \n",
    "\"San Francisco is an awesome city\"]\n",
    "\n",
    "# Create list\n",
    "tagged_tweets = []\n",
    "\n",
    "# tag each word and each tweet\n",
    "for tweet in tweets:\n",
    "    tweet_tag = nltk.pos_tag(nltk.word_tokenize(tweet))\n",
    "    tagged_tweets.append([tag for word, tag in tweet_tag])\n",
    "\n",
    "# use one-hot encoding to conver the tags into features\n",
    "one_hot_multi = MultiLabelBinarizer()\n",
    "one_hot_multi.fit_transform(tagged_tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['DT', 'IN', 'JJ', 'NN', 'NNP', 'PRP', 'VBG', 'VBP', 'VBZ'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_multi.classes_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 6.9  - Weighting Word Importance book examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x8 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 8 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# create_text \n",
    "text_data = np.array([\"I love Brazil. Brazil!\", \n",
    "\"Sweden is best\", \"Germany beats both\"])\n",
    "\n",
    "# create the tf-idf feature matrix\n",
    "tfidf = TfidfVectorizer()\n",
    "feature_matrix = tfidf.fit_transform(text_data)\n",
    "\n",
    "# show tf-idf feature matrix\n",
    "feature_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.89442719, 0.        ,\n",
       "        0.        , 0.4472136 , 0.        ],\n",
       "       [0.        , 0.57735027, 0.        , 0.        , 0.        ,\n",
       "        0.57735027, 0.        , 0.57735027],\n",
       "       [0.57735027, 0.        , 0.57735027, 0.        , 0.57735027,\n",
       "        0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show tf-idf featyre matrix as dense matrix\n",
    "feature_matrix.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'love': 6,\n",
       " 'brazil': 3,\n",
       " 'sweden': 7,\n",
       " 'is': 5,\n",
       " 'best': 1,\n",
       " 'germany': 4,\n",
       " 'beats': 0,\n",
       " 'both': 2}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show feature names\n",
    "tfidf.vocabulary_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 7.6  Encoding Days of the week - book examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Thursday\n",
       "1      Sunday\n",
       "2     Tuesday\n",
       "dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# create dates\n",
    "dates = pd.Series(pd.date_range(\"2/2/2002\", periods=3, freq=\"M\"))\n",
    "\n",
    "# show days of the week\n",
    "dates.dt.weekday_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3\n",
       "1    6\n",
       "2    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show days of the week\n",
    "dates.dt.weekday\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Part A\n",
    " Provide me with an example (besides counting words in a document) of how these techniques could be used. (Just a couple sentences.) <br><br>\n",
    " We can use the words from some source to help determine a few things besides just word count. For example, if we consider an article, we can determine the subject of the article by how often a word appears. If economy or economics appears often we can say the subject or point or the article is about economics. Similiarlly, we can determine the mood or gender by looking for key words (e.g., good, great, she, him, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Part C\n",
    " You can create a datafile file or use one from the course resources. You must use DataFrames!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Statement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Person_1</td>\n",
       "      <td>I hate the outdoors!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Person_2</td>\n",
       "      <td>The outdoors is okay.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Person_3</td>\n",
       "      <td>I can't stand to be inside! Outside is my home.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Name                                        Statement\n",
       "0  Person_1                             I hate the outdoors!\n",
       "1  Person_2                            The outdoors is okay.\n",
       "2  Person_3  I can't stand to be inside! Outside is my home."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a dataframe for Part B.\n",
    "import pandas as pd\n",
    "\n",
    "data = [[\"Person_1\", \"I hate the outdoors!\"],\n",
    "        [\"Person_2\", \"The outdoors is okay.\"],\n",
    "        [\"Person_3\", \"I can't stand to be inside! Outside is my home.\"]]\n",
    "\n",
    "df = pd.DataFrame(data, columns = ['Name', \"Statement\"])\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Part B\n",
    " Then implement at least 3 of these Text techniques in a program demonstrating how your example could be accomplished. Be sure to include lots of comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I hate the outdoors! This persons statement indicates a bad mood about the subject.\n",
      "\n",
      "The outdoors is okay. This persons statement didn't indicate a good or bad mood about the subject.\n",
      "\n",
      "I can't stand to be inside! Outside is my home. This persons statement didn't indicate a good or bad mood about the subject.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# mood and subject are what we are looking to determine from the df\n",
    "\n",
    "# create list of words for mood\n",
    "good = [\"good\", \"happy\", \"love\"]\n",
    "bad = [\"bad\", \"hate\", \"sad\"]\n",
    "\n",
    "# tokenize statements of each row to compare to list for mood (example 6.4)\n",
    "statements = df.Statement\n",
    "\n",
    "# if the tokenized word matches our mood list they get a point\n",
    "# the higher points determins the mood of the statement\n",
    "for s in statements:\n",
    "    good_mood = 0\n",
    "    bad_mood = 0\n",
    "\n",
    "    wt = word_tokenize(s)\n",
    "\n",
    "    for word in wt:\n",
    "        if word in good:\n",
    "            good_mood +=1\n",
    "        \n",
    "        elif word in bad:\n",
    "            bad_mood +=1\n",
    "    \n",
    "    if good_mood > bad_mood:\n",
    "        print(s, \"This persons statement indicates a good mood about the subject.\\n\")\n",
    "    elif bad_mood > good_mood:\n",
    "        print(s, \"This persons statement indicates a bad mood about the subject.\\n\")\n",
    "    else:\n",
    "        print(s, \"This persons statement didn't indicate a good or bad mood about the subject.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t0.680918560398684\n",
      "  (0, 11)\t0.5178561161676974\n",
      "  (0, 8)\t0.5178561161676974\n",
      "  (1, 11)\t0.45985352875883484\n",
      "  (1, 8)\t0.45985352875883484\n",
      "  (1, 5)\t0.45985352875883484\n",
      "  (1, 7)\t0.604652128305311\n",
      "  (2, 5)\t0.2596634391575384\n",
      "  (2, 1)\t0.3414262179382391\n",
      "  (2, 10)\t0.3414262179382391\n",
      "  (2, 12)\t0.3414262179382391\n",
      "  (2, 0)\t0.3414262179382391\n",
      "  (2, 4)\t0.3414262179382391\n",
      "  (2, 9)\t0.3414262179382391\n",
      "  (2, 6)\t0.3414262179382391\n",
      "  (2, 3)\t0.3414262179382391 \n",
      "\n",
      "[[0.         0.         0.68091856 0.         0.         0.\n",
      "  0.         0.         0.51785612 0.         0.         0.51785612\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.45985353\n",
      "  0.         0.60465213 0.45985353 0.         0.         0.45985353\n",
      "  0.        ]\n",
      " [0.34142622 0.34142622 0.         0.34142622 0.34142622 0.25966344\n",
      "  0.34142622 0.         0.         0.34142622 0.34142622 0.\n",
      "  0.34142622]] \n",
      "\n",
      "{'be': 0,\n",
      " 'can': 1,\n",
      " 'hate': 2,\n",
      " 'home': 3,\n",
      " 'inside': 4,\n",
      " 'is': 5,\n",
      " 'my': 6,\n",
      " 'okay': 7,\n",
      " 'outdoors': 8,\n",
      " 'outside': 9,\n",
      " 'stand': 10,\n",
      " 'the': 11,\n",
      " 'to': 12}\n"
     ]
    }
   ],
   "source": [
    "# since we also want to try to predict the subject, lets wigh the words of the statements. (example 6.9)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pprint import pprint\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "feature_matrix = tfidf.fit_transform(statements)\n",
    "print(feature_matrix, \"\\n\")\n",
    "print(feature_matrix.toarray(),\"\\n\")\n",
    "pprint(tfidf.vocabulary_)\n",
    "\n",
    "# I initially thought that the subject, outdoors, would be the most important. \n",
    "# However, since it is frequent within the document it becomes less important. \n",
    "# On the otherhand, the mood (e.g., hate, okay) become the most important word of each persons statement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I'] \n",
      "\n",
      "[] \n",
      "\n",
      "['I', 'my'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# determining parts of speach to see if the statement is about the person (example 6.7)\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "\n",
    "for s in statements:\n",
    "    text_tagged = pos_tag(word_tokenize(s))\n",
    "\n",
    "    # print personal pronouns\n",
    "    personal = [word for word, tag in text_tagged if tag in [\"PRP\", \"PRP$\"]]\n",
    "    print(personal, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
